** ComfyUI startup time: 2024-03-09 21:34:46.331222
[2024-03-09 21:34] ** Platform: Windows
[2024-03-09 21:34] ** Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]
[2024-03-09 21:34] ** Python executable: C:\Users\r_c_m\AppData\Local\Microsoft\WindowsApps\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\python.exe
[2024-03-09 21:34] ** Log path: D:\Github\ComfyUI\comfyui.log
[2024-03-09 21:34] 
Prestartup times for custom nodes:
[2024-03-09 21:34]    0.1 seconds: D:\Github\ComfyUI\custom_nodes\ComfyUI-Manager
[2024-03-09 21:34] 
[2024-03-09 21:34] Total VRAM 8192 MB, total RAM 16297 MB
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.0.1+cu118 with CUDA 1108 (you have 2.2.1+cu121)
    Python  3.10.11 (you have 3.10.11)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
[2024-03-09 21:34] xformers version: 0.0.20
[2024-03-09 21:34] Set vram state to: NORMAL_VRAM
[2024-03-09 21:34] Device: cuda:0 NVIDIA GeForce RTX 2070 : cudaMallocAsync
[2024-03-09 21:34] VAE dtype: torch.float32
[2024-03-09 21:34] Using pytorch cross attention
[2024-03-09 21:34] Adding extra search path checkpoints D:\Github\stable-diffusion-webui\models/Stable-diffusion
[2024-03-09 21:34] Adding extra search path configs D:\Github\stable-diffusion-webui\configs
[2024-03-09 21:34] Adding extra search path vae D:\Github\stable-diffusion-webui\models\VAE
[2024-03-09 21:34] Adding extra search path loras D:\Github\stable-diffusion-webui\models\Lora
[2024-03-09 21:34] Adding extra search path loras D:\Github\stable-diffusion-webui\models/LyCORIS
[2024-03-09 21:34] Adding extra search path upscale_models D:\Github\stable-diffusion-webui\models\ESRGAN
[2024-03-09 21:34] Adding extra search path upscale_models D:\Github\stable-diffusion-webui\models\RealESRGAN
[2024-03-09 21:34] Adding extra search path upscale_models D:\Github\stable-diffusion-webui\models\SwinIR
[2024-03-09 21:34] Adding extra search path embeddings D:\Github\stable-diffusion-webui\embeddings
[2024-03-09 21:34] Adding extra search path hypernetworks D:\Github\stable-diffusion-webui\models\hypernetworks
[2024-03-09 21:34] Adding extra search path controlnet D:\Github\stable-diffusion-webui\models\ControlNet
[2024-03-09 21:34] ### Loading: ComfyUI-Manager (V2.9)
[2024-03-09 21:34] ### ComfyUI Revision: 2052 [55f37baa] | Released on '2024-03-07'
[2024-03-09 21:34] 
Import times for custom nodes:
[2024-03-09 21:34]    0.5 seconds: D:\Github\ComfyUI\custom_nodes\ComfyUI-Manager
[2024-03-09 21:34] 
[2024-03-09 21:34] Starting server
[2024-03-09 21:34] 
[2024-03-09 21:34] To see the GUI go to: http://127.0.0.1:8188
[2024-03-09 21:34] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/alter-list.json
[2024-03-09 21:34] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json
[2024-03-09 21:34] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/extension-node-map.json
[2024-03-09 21:34] [ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/model-list.json
[2024-03-09 21:35] FETCH DATA from: D:\Github\ComfyUI\custom_nodes\ComfyUI-Manager\extension-node-map.json
[2024-03-09 21:36] got prompt
[2024-03-09 21:36] ERROR:root:Failed to validate prompt for output 9:
[2024-03-09 21:36] ERROR:root:* CheckpointLoaderSimple 4:
[2024-03-09 21:36] ERROR:root:  - Value not in list: ckpt_name: 'v1-5-pruned-emaonly.ckpt' not in ['beautifulRealistic_v7.safetensors', 'chilloutmix_NiPrunedFp32Fix.safetensors', 'disneyPixarCartoon_v10.safetensors', 'dreamshaper_8.safetensors', 'epicphotogasm_z.safetensors', 'epicrealism_naturalSinRC1VAE.safetensors', 'majicmixRealistic_v7.safetensors', 'onlyrealistic_v30BakedVAE.safetensors', 'realcartoon3d_v9.safetensors', 'sdhk_v30.safetensors', 'sdvn7Nijistylexl_v1.safetensors', 'toonyouJP_alpha1.safetensors', 'toonyou_beta3.safetensors', 'toonyou_beta6.safetensors', 'xxmix9realistic_v40.safetensors']
[2024-03-09 21:36] ERROR:root:Output will be ignored
[2024-03-09 21:36] invalid prompt: {'type': 'prompt_outputs_failed_validation', 'message': 'Prompt outputs failed validation', 'details': '', 'extra_info': {}}
[2024-03-09 21:37] got prompt
[2024-03-09 21:37] ERROR:root:Failed to validate prompt for output 9:
[2024-03-09 21:37] ERROR:root:* CheckpointLoaderSimple 4:
[2024-03-09 21:37] ERROR:root:  - Value not in list: ckpt_name: 'v1-5-pruned-emaonly.ckpt' not in ['beautifulRealistic_v7.safetensors', 'chilloutmix_NiPrunedFp32Fix.safetensors', 'disneyPixarCartoon_v10.safetensors', 'dreamshaper_8.safetensors', 'epicphotogasm_z.safetensors', 'epicrealism_naturalSinRC1VAE.safetensors', 'majicmixRealistic_v7.safetensors', 'onlyrealistic_v30BakedVAE.safetensors', 'realcartoon3d_v9.safetensors', 'sdhk_v30.safetensors', 'sdvn7Nijistylexl_v1.safetensors', 'toonyouJP_alpha1.safetensors', 'toonyou_beta3.safetensors', 'toonyou_beta6.safetensors', 'xxmix9realistic_v40.safetensors']
[2024-03-09 21:37] ERROR:root:Output will be ignored
[2024-03-09 21:37] invalid prompt: {'type': 'prompt_outputs_failed_validation', 'message': 'Prompt outputs failed validation', 'details': '', 'extra_info': {}}
[2024-03-09 21:46] got prompt
[2024-03-09 21:46] model_type EPS
[2024-03-09 21:46] adm 0
[2024-03-09 21:46] Using pytorch attention in VAE
[2024-03-09 21:46] Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
[2024-03-09 21:46] Using pytorch attention in VAE
[2024-03-09 21:46] clip missing: ['clip_l.logit_scale', 'clip_l.transformer.text_projection.weight']
[2024-03-09 21:46] clip unexpected: ['clip_l.transformer.text_model.embeddings.position_ids']
[2024-03-09 21:46] loaded straight to GPU
[2024-03-09 21:46] Requested to load BaseModel
[2024-03-09 21:46] Loading 1 new model
[2024-03-09 21:46] Requested to load SD1ClipModel
[2024-03-09 21:46] Loading 1 new model
[2024-03-09 21:46] D:\Github\ComfyUI\comfy\ldm\modules\attention.py:344: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)
  out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0, is_causal=False)
[2024-03-09 21:46] 100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  8.31it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  5.26it/s]
[2024-03-09 21:46] Requested to load AutoencoderKL
[2024-03-09 21:46] Loading 1 new model
[2024-03-09 21:46] Prompt executed in 13.95 seconds
[2024-03-09 21:47] got prompt
[2024-03-09 21:47] 100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  8.60it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  8.70it/s]
[2024-03-09 21:47] Prompt executed in 2.75 seconds
[2024-03-09 21:47] got prompt
[2024-03-09 21:47] 100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  8.59it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  8.66it/s]
[2024-03-09 21:47] Prompt executed in 2.72 seconds
[2024-03-09 21:48] got prompt
[2024-03-09 21:48] D:\Github\ComfyUI\comfy\k_diffusion\sampling.py:475: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return dpm_solver.dpm_solver_fast(x, dpm_solver.t(torch.tensor(sigma_max)), dpm_solver.t(torch.tensor(sigma_min)), n, eta, s_noise, noise_sampler)
[2024-03-09 21:48] 100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  8.52it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  8.60it/s]
[2024-03-09 21:48] Prompt executed in 2.72 seconds
[2024-03-09 21:48] got prompt
[2024-03-09 21:48] 100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  8.63it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  8.73it/s]
[2024-03-09 21:48] Prompt executed in 2.71 seconds
[2024-03-09 21:48] got prompt
[2024-03-09 21:48] 100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  8.68it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  8.70it/s]
[2024-03-09 21:48] Prompt executed in 2.82 seconds
[2024-03-09 21:48] got prompt
[2024-03-09 21:48] 100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:02<00:00,  8.46it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:02<00:00,  8.50it/s]
[2024-03-09 21:48] Prompt executed in 3.37 seconds
[2024-03-09 21:48] got prompt
[2024-03-09 21:48] 100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:02<00:00,  7.49it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:02<00:00,  8.36it/s]
[2024-03-09 21:48] Prompt executed in 3.42 seconds
[2024-03-09 21:48] got prompt
[2024-03-09 21:48] 0it [00:00, ?it/s]D:\Github\ComfyUI\comfy\k_diffusion\sampling.py:487: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x, info = dpm_solver.dpm_solver_adaptive(x, dpm_solver.t(torch.tensor(sigma_max)), dpm_solver.t(torch.tensor(sigma_min)), order, rtol, atol, h_init, pcoeff, icoeff, dcoeff, accept_safety, eta, s_noise, noise_sampler)
[2024-03-09 21:48] 2it [00:00,  9.45it/s]3it [00:00,  8.29it/s]4it [00:00,  8.37it/s]5it [00:00,  8.44it/s]6it [00:00,  8.38it/s]7it [00:00,  8.31it/s]8it [00:00,  8.57it/s]9it [00:01,  8.64it/s]10it [00:01,  8.62it/s]11it [00:01,  8.61it/s]12it [00:01,  8.66it/s]13it [00:01,  8.62it/s]14it [00:01,  8.59it/s]15it [00:01,  8.61it/s]16it [00:01,  8.32it/s]17it [00:01,  8.59it/s]18it [00:02,  8.58it/s]19it [00:02,  8.64it/s]20it [00:02,  8.57it/s]21it [00:02,  8.60it/s]22it [00:02,  8.53it/s]23it [00:02,  8.52it/s]24it [00:02,  8.57it/s]25it [00:02,  8.53it/s]26it [00:03,  8.53it/s]27it [00:03,  8.59it/s]28it [00:03,  8.47it/s]29it [00:03,  8.48it/s]30it [00:03,  8.46it/s]31it [00:03,  8.49it/s]32it [00:03,  8.55it/s]33it [00:03,  8.46it/s]34it [00:03,  8.52it/s]35it [00:04,  8.53it/s]36it [00:04,  8.57it/s]37it [00:04,  8.44it/s]38it [00:04,  8.50it/s]39it [00:04,  8.55it/s]40it [00:04,  8.34it/s]41it [00:04,  8.56it/s]42it [00:04,  8.62it/s]43it [00:05,  8.53it/s]44it [00:05,  8.54it/s]45it [00:05,  8.60it/s]46it [00:05,  8.50it/s]47it [00:05,  8.55it/s]48it [00:05,  8.47it/s]49it [00:05,  8.54it/s]50it [00:05,  8.56it/s]51it [00:05,  8.46it/s]52it [00:06,  8.54it/s]53it [00:06,  8.54it/s]54it [00:06,  8.57it/s]55it [00:06,  8.57it/s]56it [00:06,  8.53it/s]57it [00:06,  8.57it/s]58it [00:06,  8.53it/s]59it [00:06,  8.56it/s]60it [00:07,  8.56it/s]61it [00:07,  8.52it/s]62it [00:07,  8.52it/s]63it [00:07,  8.54it/s]64it [00:07,  8.51it/s]65it [00:07,  8.46it/s]66it [00:07,  8.35it/s]67it [00:07,  8.61it/s]68it [00:07,  8.47it/s]69it [00:08,  8.48it/s]70it [00:08,  8.46it/s]71it [00:08,  8.51it/s]72it [00:08,  8.53it/s]73it [00:08,  8.46it/s]74it [00:08,  8.55it/s]75it [00:08,  8.56it/s]76it [00:08,  8.54it/s]77it [00:09,  8.54it/s]78it [00:09,  8.58it/s]78it [00:09,  8.47it/s]
[2024-03-09 21:48] Prompt executed in 9.57 seconds
[2024-03-09 21:49] got prompt
[2024-03-09 21:49] 100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:03<00:00,  7.02it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:03<00:00,  7.31it/s]
[2024-03-09 21:49] Prompt executed in 3.79 seconds
[2024-03-09 21:50] got prompt
[2024-03-09 21:50] 100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.42it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.39it/s]
[2024-03-09 21:50] Prompt executed in 12.50 seconds
[2024-03-09 21:51] got prompt
[2024-03-09 21:51] Requested to load SD1ClipModel
[2024-03-09 21:51] Loading 1 new model
[2024-03-09 21:51] Requested to load BaseModel
[2024-03-09 21:51] Loading 1 new model
[2024-03-09 21:51] 100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.42it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.43it/s]
[2024-03-09 21:51] Prompt executed in 12.75 seconds
[2024-03-09 21:52] got prompt
[2024-03-09 21:52] Requested to load BaseModel
[2024-03-09 21:52] Loading 1 new model
[2024-03-09 21:52] 100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.42it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.47it/s]
[2024-03-09 21:52] Prompt executed in 12.64 seconds
[2024-03-09 21:57] FETCH DATA from: D:\Github\ComfyUI\custom_nodes\ComfyUI-Manager\.cache\1514988643_custom-node-list.json
[2024-03-09 21:57] Start fetching...[2KFetching: D:\Github\ComfyUI\custom_nodes\ComfyUI-Manager[2KFetching done.
[2024-03-09 21:57] FETCH DATA from: D:\Github\ComfyUI\custom_nodes\ComfyUI-Manager\.cache\2259715867_alter-list.json
[2024-03-09 21:57] FETCH DATA from: D:\Github\ComfyUI\custom_nodes\ComfyUI-Manager\.cache\1514988643_custom-node-list.json
[2024-03-09 21:58] got prompt
[2024-03-09 21:58] Requested to load SD1ClipModel
[2024-03-09 21:58] Loading 1 new model
[2024-03-09 21:58] Requested to load BaseModel
[2024-03-09 21:58] Loading 1 new model
[2024-03-09 21:58] 100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.38it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.45it/s]
[2024-03-09 21:58] Prompt executed in 13.08 seconds
[2024-03-09 21:58] got prompt
[2024-03-09 21:58] model_type EPS
[2024-03-09 21:58] adm 0
[2024-03-09 21:58] Using pytorch attention in VAE
[2024-03-09 21:58] Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
[2024-03-09 21:58] Using pytorch attention in VAE
[2024-03-09 21:58] clip missing: ['clip_l.logit_scale', 'clip_l.transformer.text_projection.weight']
[2024-03-09 21:58] clip unexpected: ['clip_l.transformer.text_model.embeddings.position_ids']
[2024-03-09 21:58] left over keys: dict_keys(['embedding_manager.embedder.transformer.text_model.embeddings.position_embedding.weight', 'embedding_manager.embedder.transformer.text_model.embeddings.position_ids', 'embedding_manager.embedder.transformer.text_model.embeddings.token_embedding.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.0.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.1.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.10.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.11.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.2.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.3.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.4.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.5.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.6.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.7.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.8.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.layer_norm1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.layer_norm1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.layer_norm2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.layer_norm2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.mlp.fc1.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.mlp.fc1.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.mlp.fc2.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.mlp.fc2.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.self_attn.k_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.self_attn.k_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.self_attn.out_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.self_attn.out_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.self_attn.q_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.self_attn.q_proj.weight', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.self_attn.v_proj.bias', 'embedding_manager.embedder.transformer.text_model.encoder.layers.9.self_attn.v_proj.weight', 'embedding_manager.embedder.transformer.text_model.final_layer_norm.bias', 'embedding_manager.embedder.transformer.text_model.final_layer_norm.weight', 'lora_te_text_model_encoder_layers_0_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_0_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_0_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_0_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_0_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_0_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_0_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_0_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_0_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_0_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_0_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_0_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_0_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_0_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_0_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_0_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_10_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_10_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_10_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_10_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_10_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_10_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_10_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_10_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_10_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_10_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_10_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_10_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_10_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_10_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_10_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_10_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_10_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_10_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_11_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_11_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_11_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_11_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_11_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_11_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_11_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_11_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_11_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_11_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_11_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_11_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_11_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_11_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_11_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_11_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_11_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_11_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_1_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_1_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_1_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_1_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_1_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_1_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_1_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_1_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_1_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_1_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_1_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_1_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_1_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_1_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_1_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_1_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_1_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_1_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_2_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_2_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_2_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_2_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_2_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_2_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_2_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_2_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_2_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_2_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_2_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_2_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_2_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_2_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_2_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_2_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_2_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_2_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_3_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_3_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_3_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_3_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_3_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_3_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_3_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_3_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_3_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_3_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_3_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_3_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_3_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_3_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_3_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_3_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_3_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_3_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_4_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_4_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_4_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_4_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_4_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_4_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_4_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_4_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_4_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_4_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_4_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_4_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_4_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_4_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_4_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_4_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_4_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_4_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_5_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_5_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_5_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_5_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_5_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_5_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_5_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_5_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_5_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_5_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_5_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_5_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_5_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_5_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_5_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_5_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_5_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_5_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_6_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_6_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_6_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_6_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_6_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_6_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_6_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_6_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_6_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_6_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_6_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_6_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_6_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_6_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_6_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_6_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_6_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_6_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_7_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_7_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_7_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_7_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_7_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_7_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_7_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_7_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_7_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_7_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_7_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_7_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_7_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_7_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_7_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_7_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_7_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_7_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_8_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_8_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_8_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_8_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_8_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_8_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_8_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_8_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_8_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_8_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_8_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_8_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_8_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_8_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_8_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_8_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_8_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_8_self_attn_v_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_9_mlp_fc1.alpha', 'lora_te_text_model_encoder_layers_9_mlp_fc1.lora_down.weight', 'lora_te_text_model_encoder_layers_9_mlp_fc1.lora_up.weight', 'lora_te_text_model_encoder_layers_9_mlp_fc2.alpha', 'lora_te_text_model_encoder_layers_9_mlp_fc2.lora_down.weight', 'lora_te_text_model_encoder_layers_9_mlp_fc2.lora_up.weight', 'lora_te_text_model_encoder_layers_9_self_attn_k_proj.alpha', 'lora_te_text_model_encoder_layers_9_self_attn_k_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_9_self_attn_k_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_9_self_attn_out_proj.alpha', 'lora_te_text_model_encoder_layers_9_self_attn_out_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_9_self_attn_out_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_9_self_attn_q_proj.alpha', 'lora_te_text_model_encoder_layers_9_self_attn_q_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_9_self_attn_q_proj.lora_up.weight', 'lora_te_text_model_encoder_layers_9_self_attn_v_proj.alpha', 'lora_te_text_model_encoder_layers_9_self_attn_v_proj.lora_down.weight', 'lora_te_text_model_encoder_layers_9_self_attn_v_proj.lora_up.weight'])
[2024-03-09 21:58] loaded straight to GPU
[2024-03-09 21:58] Requested to load BaseModel
[2024-03-09 21:58] Loading 1 new model
[2024-03-09 21:58] Requested to load SD1ClipModel
[2024-03-09 21:58] Loading 1 new model
[2024-03-09 21:59] 100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.43it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.48it/s]
[2024-03-09 21:59] Requested to load AutoencoderKL
[2024-03-09 21:59] Loading 1 new model
[2024-03-09 21:59] Prompt executed in 20.72 seconds
[2024-03-09 22:02] got prompt
[2024-03-09 22:02] Requested to load BaseModel
[2024-03-09 22:02] Loading 1 new model
[2024-03-09 22:02] 100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.39it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.47it/s]
[2024-03-09 22:02] Prompt executed in 12.78 seconds
[2024-03-09 22:02] got prompt
[2024-03-09 22:02] Requested to load BaseModel
[2024-03-09 22:02] Loading 1 new model
[2024-03-09 22:02] 100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.39it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.46it/s]
[2024-03-09 22:02] Prompt executed in 12.70 seconds
[2024-03-09 22:02] got prompt
[2024-03-09 22:02] model_type EPS
[2024-03-09 22:02] adm 0
[2024-03-09 22:02] Using pytorch attention in VAE
[2024-03-09 22:02] Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
[2024-03-09 22:02] Using pytorch attention in VAE
[2024-03-09 22:02] clip missing: ['clip_l.logit_scale', 'clip_l.transformer.text_projection.weight']
[2024-03-09 22:02] clip unexpected: ['clip_l.transformer.text_model.embeddings.position_ids']
[2024-03-09 22:02] left over keys: dict_keys(['model_ema.decay', 'model_ema.num_updates'])
[2024-03-09 22:02] loaded straight to GPU
[2024-03-09 22:02] Requested to load BaseModel
[2024-03-09 22:02] Loading 1 new model
[2024-03-09 22:02] Requested to load SD1ClipModel
[2024-03-09 22:02] Loading 1 new model
[2024-03-09 22:02] 100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.42it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.47it/s]
[2024-03-09 22:02] Requested to load AutoencoderKL
[2024-03-09 22:02] Loading 1 new model
[2024-03-09 22:02] Prompt executed in 18.41 seconds
[2024-03-09 22:03] got prompt
[2024-03-09 22:03] Requested to load BaseModel
[2024-03-09 22:03] Loading 1 new model
[2024-03-09 22:03] 100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.40it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.48it/s]
[2024-03-09 22:03] Prompt executed in 12.74 seconds
[2024-03-09 22:04] got prompt
[2024-03-09 22:04] Requested to load SD1ClipModel
[2024-03-09 22:04] Loading 1 new model
[2024-03-09 22:04] Requested to load BaseModel
[2024-03-09 22:04] Loading 1 new model
[2024-03-09 22:04] 100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  8.67it/s]100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  8.85it/s]
[2024-03-09 22:04] Prompt executed in 3.27 seconds
[2024-03-09 22:06] FETCH DATA from: D:\Github\ComfyUI\custom_nodes\ComfyUI-Manager\.cache\2259715867_alter-list.json
[2024-03-09 22:06] FETCH DATA from: D:\Github\ComfyUI\custom_nodes\ComfyUI-Manager\.cache\1514988643_custom-node-list.json
[2024-03-09 22:07] 
Stopped server
